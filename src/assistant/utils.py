import os
import requests
from typing import Dict, Any
from langsmith import traceable
from tavily import TavilyClient
from assistant.state import SummaryState  # ì´ ì¤„ì„ ì¶”ê°€
import hashlib
import pickle
import os
# ì‹œê°„ ëª¨ë“ˆ ì„í¬íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤ (ì²« ë¶€ë¶„ì— ì¶”ê°€)
import time
from datetime import datetime, timedelta
import streamlit as st


class RequestLimiter:
    """API ìš”ì²­ ë¹ˆë„ ì œí•œ ë° ìºì‹±"""
    def __init__(self, cache_dir="api_cache", max_requests_per_minute=50):
        self.cache_dir = cache_dir
        self.max_requests = max_requests_per_minute
        self.request_times = []
        self.cache = {}
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        
        # ê¸°ì¡´ ìºì‹œ ë¡œë“œ
        try:
            for filename in os.listdir(cache_dir):
                if filename.endswith(".cache"):
                    with open(os.path.join(cache_dir, filename), "rb") as f:
                        self.cache[filename[:-6]] = pickle.load(f)
        except Exception as e:
            print(f"ìºì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _get_cache_key(self, prompt):
        """í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(prompt.encode()).hexdigest()
    
    def can_make_request(self):
        """ìš”ì²­ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        now = datetime.now()
        # 1ë¶„ ì´ìƒ ì§€ë‚œ ìš”ì²­ ì œê±°
        self.request_times = [t for t in self.request_times if now - t < timedelta(minutes=1)]
        return len(self.request_times) < self.max_requests
    
    def wait_for_capacity(self):
        """ìš”ì²­ ê°€ëŠ¥í•  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        while not self.can_make_request():
            time.sleep(0.5)
    
    def get_llm_response(self, llm, messages, force_refresh=False):
        """LLM ì‘ë‹µ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ìš°ì„ )"""
        # ìºì‹œ í‚¤ ìƒì„±
        prompt = "".join(msg.content for msg in messages)
        cache_key = self._get_cache_key(prompt)
        
        # ìºì‹œì—ì„œ ì°¾ê¸°
        if not force_refresh and cache_key in self.cache:
            return self.cache[cache_key]
        
        # ìš©ëŸ‰ í™•ì¸ ë° ëŒ€ê¸°
        self.wait_for_capacity()
        
        # ìš”ì²­ ì‹œê°„ ê¸°ë¡
        self.request_times.append(datetime.now())
        
        # LLM í˜¸ì¶œ
        try:
            response = llm.invoke(messages)
            self.cache[cache_key] = response
            
            # ìºì‹œ ì €ì¥
            try:
                with open(os.path.join(self.cache_dir, f"{cache_key}.cache"), "wb") as f:
                    pickle.dump(response, f)
            except Exception as e:
                print(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
                
            return response
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìºì‹œì—ì„œ ì œê±°
            if cache_key in self.cache:
                del self.cache[cache_key]
            raise e


def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=False):
    """
    Takes either a single search response or list of responses from search APIs and formats them.
    Limits the raw_content to approximately max_tokens_per_source.
    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.
    
    Args:
        search_response: Either:
            - A dict with a 'results' key containing a list of search results
            - A list of dicts, each containing search results
            
    Returns:
        str: Formatted string with deduplicated sources
    """
    # Convert input to list of results
    if isinstance(search_response, dict):
        sources_list = search_response['results']
    elif isinstance(search_response, list):
        sources_list = []
        for response in search_response:
            if isinstance(response, dict) and 'results' in response:
                sources_list.extend(response['results'])
            else:
                sources_list.extend(response)
    else:
        raise ValueError("Input must be either a dict with 'results' or a list of search results")
    
    # Deduplicate by URL
    unique_sources = {}
    for source in sources_list:
        if source['url'] not in unique_sources:
            unique_sources[source['url']] = source
    
    # Format output
    formatted_text = "Sources:\n\n"
    for i, source in enumerate(unique_sources.values(), 1):
        formatted_text += f"Source {source['title']}:\n===\n"
        formatted_text += f"URL: {source['url']}\n===\n"
        formatted_text += f"Most relevant content from source: {source['content']}\n===\n"
        if include_raw_content:
            # Using rough estimate of 4 characters per token
            char_limit = max_tokens_per_source * 4
            # Handle None raw_content
            raw_content = source.get('raw_content', '')
            if raw_content is None:
                raw_content = ''
                print(f"Warning: No raw_content found for source {source['url']}")
            if len(raw_content) > char_limit:
                raw_content = raw_content[:char_limit] + "... [truncated]"
            formatted_text += f"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\n\n"
                
    return formatted_text.strip()

def format_sources(search_results):
    """Format search results into a bullet-point list of sources.
    
    Args:
        search_results: List of search results or dict with 'results' key
        
    Returns:
        str: Formatted string with sources and their URLs
    """
    if not search_results:
        return "ì°¸ê³  ë¬¸í—Œ ì—†ìŒ"
        
    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì§ì ‘ ì²˜ë¦¬
    if isinstance(search_results, list):
        formatted_sources = []
        for result in search_results:
            if isinstance(result, dict) and 'results' in result:
                # ë”•ì…”ë„ˆë¦¬ ì•ˆì˜ results ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
                for source in result['results']:
                    formatted_sources.append(f"* {source['title']} : {source['url']}")
            elif isinstance(result, str):
                # ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶”ê°€
                formatted_sources.append(result)
        return '\n'.join(formatted_sources)
    
    # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° results í‚¤ë¡œ ì ‘ê·¼
    elif isinstance(search_results, dict) and 'results' in search_results:
        return '\n'.join(
            f"* {source['title']} : {source['url']}"
            for source in search_results['results']
        )
    
    return "ì°¸ê³  ë¬¸í—Œ í˜•ì‹ ì˜¤ë¥˜"

@traceable
def tavily_search(query, include_raw_content=True, max_results=3):
    """ Search the web using the Tavily API.
    
    Args:
        query (str): The search query to execute
        include_raw_content (bool): Whether to include the raw_content from Tavily in the formatted string
        max_results (int): Maximum number of results to return
        
    Returns:
        dict: Search response containing:
            - results (list): List of search result dictionaries, each containing:
                - title (str): Title of the search result
                - url (str): URL of the search result
                - content (str): Snippet/summary of the content
                - raw_content (str): Full content of the page if available"""
     
    tavily_client = TavilyClient()
    return tavily_client.search(query, 
                         max_results=max_results, 
                         include_raw_content=include_raw_content)

@traceable
def perplexity_search(query: str, perplexity_search_loop_count: int) -> Dict[str, Any]:
    """Search the web using the Perplexity API.
    
    Args:
        query (str): The search query to execute
        perplexity_search_loop_count (int): The loop step for perplexity search (starts at 0)
  
    Returns:
        dict: Search response containing:
            - results (list): List of search result dictionaries, each containing:
                - title (str): Title of the search result
                - url (str): URL of the search result
                - content (str): Snippet/summary of the content
                - raw_content (str): Full content of the page if available
    """

    headers = {
        "accept": "application/json",
        "content-type": "application/json",
        "Authorization": f"Bearer {os.getenv('PERPLEXITY_API_KEY')}"
    }
    
    payload = {
        "model": "sonar-pro",
        "messages": [
            {
                "role": "system",
                "content": "Search the web and provide factual information with sources."
            },
            {
                "role": "user",
                "content": query
            }
        ]
    }
    
    response = requests.post(
        "https://api.perplexity.ai/chat/completions",
        headers=headers,
        json=payload
    )
    response.raise_for_status()  # Raise exception for bad status codes
    
    # Parse the response
    data = response.json()
    content = data["choices"][0]["message"]["content"]

    # Perplexity returns a list of citations for a single search result
    citations = data.get("citations", ["https://perplexity.ai"])
    
    # Return first citation with full content, others just as references
    results = [{
        "title": f"Perplexity Search {perplexity_search_loop_count + 1}, Source 1",
        "url": citations[0],
        "content": content,
        "raw_content": content
    }]
    
    # Add additional citations without duplicating content
    for i, citation in enumerate(citations[1:], start=2):
        results.append({
            "title": f"Perplexity Search {perplexity_search_loop_count + 1}, Source {i}",
            "url": citation,
            "content": "See above for full content",
            "raw_content": None
        })
    
    return {"results": results}


# ì „ì—­ ë³€ìˆ˜ë¡œ ì„¸ì…˜ë³„ íŒŒì¼ ê²½ë¡œ ì €ì¥
_session_files: Dict[str, str] = {}

def save_research_process(state: SummaryState, step_name: str, step_content: str = None):
    """Save each step of the research process to a file, one file per research session.
    
    Args:
        state (SummaryState): The current state
        step_name (str): Name of the current step (e.g., 'query', 'search_results')
        step_content (str, optional): Additional content to save
    """
    try:
        # ì—°êµ¬ ì£¼ì œë¥¼ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì„¸ì…˜ IDë¡œ ì‚¬ìš©
        session_id = hash(state.research_topic)
        
        # ì´ ì„¸ì…˜ì˜ ì²« ì €ì¥ì¸ ê²½ìš° ìƒˆ íŒŒì¼ ìƒì„±
        if session_id not in _session_files:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            topic_slug = "".join(c if c.isalnum() else "_" for c in state.research_topic)[:30]
            filename = f"{topic_slug}_{timestamp}_research_process.md"
            
            # Streamlit Cloudì—ì„œë„ ì‘ë™í•˜ë„ë¡ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
            os.makedirs('research_results', exist_ok=True)
            filepath = os.path.join('research_results', filename)
            _session_files[session_id] = filepath
            
            # ìƒˆ íŒŒì¼ ìƒì„± ë° í—¤ë” ì‘ì„±
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# Research Process: {state.research_topic}\n")
                f.write(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
            print(f"\nNew research session started. Results will be saved to: {filepath}")
        
        # í˜„ì¬ ì„¸ì…˜ì˜ íŒŒì¼ì— ë‚´ìš© ì¶”ê°€
        filepath = _session_files[session_id]
        log_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        content = f"\n## {step_name} - {log_timestamp}\n"
        if step_content:
            content += f"{step_content}\n"
        
        with open(filepath, 'a', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        # Streamlit Cloudì—ì„œ íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—ëŸ¬ ì²˜ë¦¬
        print(f"ë¡œê·¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (Streamlit Cloudì—ì„œëŠ” ì •ìƒ): {e}")
        pass

def clear_session_files():
    """Clear the session files dictionary. 
    Call this when you want to force a new file for the same topic."""
    _session_files.clear()

# ë…¸ë“œ ìƒíƒœ ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ë“¤ ì¶”ê°€ (íŒŒì¼ ëì— ì¶”ê°€)
def update_node_status(node_name, status, content=None):
    """ë…¸ë“œ ìƒíƒœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
    
    Args:
        node_name (str): í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë…¸ë“œ ì´ë¦„
        status (str): í˜„ì¬ ìƒíƒœ (ì˜ˆ: "ì‹œì‘", "ì™„ë£Œ", "ì˜¤ë¥˜")
        content (str, optional): ë…¸ë“œ ì²˜ë¦¬ ê²°ê³¼ ë‚´ìš©
    """
    if "node_history" not in st.session_state:
        st.session_state.node_history = []
    
    timestamp = datetime.now().strftime("%H:%M:%S")
    
    # ìƒíƒœ ì •ë³´ ì €ì¥
    node_info = {
        "timestamp": timestamp,
        "node": node_name,
        "status": status,
        "content": content
    }
    
    # ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
    st.session_state.node_history.append(node_info)
    
    # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë…¸ë“œ ì—…ë°ì´íŠ¸
    st.session_state.current_node = node_name
    
    # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
    print(f"[{timestamp}] ë…¸ë“œ {node_name}: {status}")

def get_node_status_emoji(node_name):
    """ë…¸ë“œ ì´ë¦„ì— ë”°ë¥¸ ì´ëª¨ì§€ ë°˜í™˜"""
    node_emojis = {
        "initialize": "ğŸ”",
        "reason_from_sources": "ğŸ§ ", 
        "web_research": "ğŸŒ",
        "query_mind_map": "ğŸ”„",
        "finalize_summary": "ğŸ“"
    }
    return node_emojis.get(node_name, "âš™ï¸")
    
# ì „ì—­ ìš”ì²­ ì œí•œê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
global_request_limiter = RequestLimiter(cache_dir="api_cache", max_requests_per_minute=45)


